# Предиктивная аналитика больших данных

Учебный проект для демонстрации основных этапов жизненного цикла проекта предиктивной аналитики.  

## 1. Для клонирования этого проекта на локальную машину с использованием `PyCharm`.
\
## Installation 

Клонируйте репозиторий, создайте виртуальное окружение, активируйте и установите зависимости:  

```sh
git clone https://github.com/klantsova/pabd24
cd pabd24
python -m venv venv

source venv/bin/activate  # mac or linux
.\venv\Scripts\activate   # windows

pip install -r requirements.txt
```

## Usage

### 1. Сбор данных о ценах на недвижимость 
* Создайте в папке `data` директорию `raw` для добавления туда "сырых" данных.
* В `parse_cian.py` поменять следующие параметры для скачивания:

    * `n_rooms` - количество комнат,
    * `end_page` - на какой странице остановить парсинг.
* Запустите код по парсингу данных о недвижимости с ЦИАН:
  
```sh
python .src\parse_cian.py
```

В итоге вы получите сырые данные по недвижимости, которые у вас будут в папке `data/raw`.

### 2. Выгрузка данных в хранилище S3 
Для доступа к хранилищу скопируйте файл `.env` в корень проекта:
* Скачать файл [env](https://drive.google.com/drive/folders/1cUry7oySkAJ5OB5lMGQcMceTO2nWxUHT?usp=drive_link) с диска.
* Переименовать его с env на **.env** (*добавить точку*) и перенести к себе в корень проекта.

Для выгрузки сырых данных в хранилище S3:
* В `upload_to_s3.py` поменять следующие параметры:

    * `YOUR_ID` - твой порядковый номер в списке учащихся (твоя папка в бакете),
    * `CSV_PATH` - список путей к сырым файлам (необходимо поменять названия).
* Запустить код по перемещению данных:
  
```sh
python .src\upload_to_s3.py
```

В итоге в хранилище появится ваши данные. \
Их наличие можно проверить по [ссылке](https://storage.yandexcloud.net/pabd24). \
\
При успешной загрузке они должны отобразиться в таком виде: `<Key>23/data/raw/1_2024-05-14_19-29.csv</Key>` (*где **23** - твой id, а **1_2024-05-14_19-29** - название файла*).

### 3. Загрузка данных из S3 на локальную машину  
*После того, как ваши файлы были загружены в бакет s3 их можно не парсить снова, а просто загружать из хранилища на локальную машину.*
* Если вы не создавали сырые данные (не парсили), то на этом этапе надо создать в папке `data` директорию `raw` для загрузки данных с бакета.
* В `download_to_s3.py` поменять следующие параметры:

    * `YOUR_ID` - твой порядковый номер в списке учащихся (твоя папка в бакете),
    * `CSV_PATH` - список путей к файлам (необходимо поменять названия).
* Запустить код по перемещению данных:
  
```sh
python .src\download_to_s3.py
```

В итоге вы получите свои сырые данные, скаченные с бакета, в папке `data/raw`.

### 4. Предварительная обработка данных  
* В папке `data` создать директорию `proc` для последующего перемещения туда предобработанных данных (для моделирования).
* В `preprocess_data.py` нужно поменять следующие параметры:

    * `IN_FILES` - список путей к файлам (необходимо поменять названия),
    * `TRAIN_SIZE` - размер тренировочной выборки (доля от общего размера входных данных),
    * `delimiter` - разделитель при прочтение данных (*скорее всего: либо **','**, либо **';'***),
    * `30_000_000` - цена, по которой обрезаются данные (не берутся квартиры дороже, чем это значение).
* Запустить код по предварительной обработке данных:

```sh
python .src\preprocess_data.py
```
В итоге вы получите предобработанные данные, разделенные на тренировочную и валидационную выборки, в папке `data/proc`. \
В логах также можно увидеть названия входных данных и получившихся после предобработки выборок, а также размер тренировочной выборки.

### 5. Обучение модели (и тестирование)
1) **Описание модели и входных параметров для предсказания**:\
Линейная регрессия, где *цена* это зависимая переменная, а *квадратные метры* - регрессор, так же в спецификацию модели добавлена константа.\
В качестве метрик качества модели выбраны показатели R2.
2) **Параметры модели, которые можно изменить**:

    * `MODEL_SAVE_PATH` - путь, куда сохранится обученная модель (необходимо поменять название).
* После настройки всех параметров надо запустить код по предварительной обработке данных:

```sh
python .src\train_model.py
```
В итоге вы получите обученную модель, которая сохраняется (ее веса) в папке `models`. \
В логах также можно увидеть R2 и уравнение регрессии (модель). \
\
Ее (чтобы потом не обучать заново) можно так же выгрузить в хранилище s3, а потом скачивать обученную модель для предсказаний из бакета:
```sh
python .src\upload_to_s3.py -i models\linear_regression_v01.joblib # для выгрузки модели в хранилище s3 (название модели поменять на нужное)
python .src\download_to_s3.py -i models\linear_regression_v01.joblib # для загрузки модели из хранилища s3 на локальную машину (название модели поменять на нужное)
```
3) **Логика валидации модели была выделена в отдельный скрипт `test_model.py`**:
На данном этапе вы проверяете свою обученную модель на валидационной выборке. В качестве метрики качества выбран MAE.

```sh
python .src\test_model.py
```
В логах также можно увидеть MAE на валидационной выборке для вашей модели.

### 6. Запуск приложения flask 
Для запуска приложения flask для предсказания значений по вашей модели:

```sh
python .src\predict_app.py
```
**Обратите внимание, что на этом этапе надо добавить `APP_TOKEN` в `.env` для верной аутентификации по токену**. \
После запуска вы увидите две ссылки - надо перейти на одну из них и попробовать ввести любое значение кв метров и токен. \
Вы получите предсказанное значение стоимости квартиры такого размера.
\
Также для тестирования сервиса с помощью библиотеки request можно использовать:

```sh
python .test\test_app.py
```

### 7. Использование сервиса через веб интерфейс 
Для использования сервиса используйте файл `web/index.html`. \
**Проверьте, чтобы в `endpoint` стоял ваш локальный хост**.

## 2. Для клонирования этого проекта на виртуальную машину.




